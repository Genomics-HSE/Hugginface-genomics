include 'configs/big_config.gin'

checkpoint_path = "RM.ckpt"
DatasetPL.batch_size = 32

train_model.model = @ReformerLabeler()
train_model.checkpoint_path = %checkpoint_path

##########################
#  Model
##########################
ReformerLabeler.embedding = @Embedding()

Embedding.num_embeddings = 2
Embedding.embedding_dim = 16

ConvEmbedding.n_layers = 4
ConvEmbedding.in_channels = 1
ConvEmbedding.out_channels = 16
ConvEmbedding.kernel_size = 21
ConvEmbedding.stride = 1

ReformerLabeler.config = @ReformerConfig()
ReformerConfig.attention_head_size = 16
ReformerConfig.attn_layers  = ["local", "lsh", "local", "lsh", "local", "lsh"]
ReformerConfig.axial_pos_embds_dim = [6, 10]  # sum is equal to hidden_size
ReformerConfig.axial_pos_shape = [128, 256] # product max_embedding_size [128, 256]
ReformerConfig.chunk_size_lm_head = 0
ReformerConfig.eos_token_id = None
ReformerConfig.feed_forward_size = 16
ReformerConfig.hidden_size = 16
ReformerConfig.max_position_embeddings = 32768
ReformerConfig.num_attention_heads = 2
ReformerConfig.use_cache = False
ReformerConfig.vocab_size = 2
ReformerConfig.pad_token_id = None
ReformerConfig.return_dict = False

ReformerConfig.local_chunk_length = 10000
ReformerConfig.local_num_chunks_before = 1
ReformerConfig.local_num_chunks_after = 1

ReformerConfig.num_hashes = 2
ReformerConfig.lsh_attn_chunk_length = 128
ReformerConfig.lsh_num_chunks_before = 1
ReformerConfig.lsh_num_chunks_after = 1

ReformerLabeler.predictor = @Predictor()
Predictor.d_model = 32
Predictor.dropout = 0.1
Predictor.n_class = 32

##########################
#  Testing
##########################

test_model.checkpoint_path = "RM_trained.ckpt"
test_model.model = @ReformerLabeler()
